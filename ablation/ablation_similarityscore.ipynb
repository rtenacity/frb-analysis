{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91526da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n",
      "Number of NaN values in 'Repeater' column before processing: 443\n",
      "Number of NaN values in 'Repeater' column after processing: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, confusion_matrix\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, recall_score\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import optuna\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "import umap.umap_ as umap\n",
    "import matplotlib\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from os.path import join\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "        \n",
    "set_seed(42)\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'font.size':           12,\n",
    "    # Titles, labels, ticks, legends all at 12â€‰pt\n",
    "    'axes.titlesize':      12,\n",
    "    'axes.labelsize':      12,\n",
    "    'xtick.labelsize':     12,\n",
    "    'ytick.labelsize':     12,\n",
    "    'legend.fontsize':     8,\n",
    "    'figure.titlesize':    12,\n",
    "    # (Optional) ensure a LaTeX package for scalable fonts\n",
    "    'text.latex.preamble': r'\\usepackage{lmodern}'\n",
    "})\n",
    "def fill_repeater_from_source(row, data):\n",
    "    if row['Source'] == 'FRB20220912A':\n",
    "        return 1\n",
    "    else:\n",
    "        return row['Repeater']\n",
    "frb_data = pd.read_csv('frb-data.csv')\n",
    "frb_data['Repeater'] = frb_data['Repeater'].map({'Yes': 1, 'No': 0})\n",
    "frb_data['Repeater'] = frb_data['Repeater'].fillna(0)\n",
    "frb_data['Repeater'] = frb_data['Repeater'].astype(int)\n",
    "frb_data['Repeater'] = frb_data.apply(fill_repeater_from_source, axis=1, data=frb_data)\n",
    "\n",
    "frb_data['Repeater'].isna().sum()\n",
    "labels = frb_data['Repeater']\n",
    "\n",
    "# Function to clean numerical strings and convert to float\n",
    "def clean_numeric_value(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        if not value:\n",
    "            return np.nan\n",
    "        try:\n",
    "            # Remove special characters and split if necessary\n",
    "            for char in ['/', '+', '<', '>', '~']:\n",
    "                value = value.replace(char, '')\n",
    "            if '-' in value:\n",
    "                value = value.split('-')[0]\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "    \n",
    "error_features = [\n",
    "    'DM_SNR', 'DM_alig', 'Flux_density', 'Fluence', 'Energy',\n",
    "    'Polar_l', 'Polar_c', 'RM_syn', 'RM_QUfit', 'Scatt_t', \n",
    "    #'Scin_f'\n",
    "]\n",
    "base_features = [\n",
    "    'Observing_band', \n",
    "    # 'GL', 'GB', \n",
    "    \n",
    "    'SNR', \n",
    "    'Freq_high',\n",
    "    'Freq_low', 'Freq_peak', \n",
    "    'Width'\n",
    "    # 'Repeater',\n",
    "    #'MJD'\n",
    "]\n",
    "\n",
    "for feature in base_features + error_features:\n",
    "    frb_data[feature] = frb_data[feature].apply(clean_numeric_value)\n",
    "\n",
    "for feature in error_features:\n",
    "    frb_data[f'{feature}_err'] = frb_data[f'{feature}_err'].apply(clean_numeric_value)\n",
    "\n",
    "for feature in error_features:\n",
    "    frb_data[f'{feature}_upper'] = frb_data[feature] + frb_data[f'{feature}_err']\n",
    "    frb_data[f'{feature}_lower'] = frb_data[feature] - frb_data[f'{feature}_err']\n",
    "    frb_data[f'{feature}_lower'] = frb_data[f'{feature}_lower'].clip(lower=0)\n",
    "\n",
    "features = (\n",
    "    base_features +\n",
    "    error_features +\n",
    "    [f'{feature}_upper' for feature in error_features] +\n",
    "    [f'{feature}_lower' for feature in error_features]\n",
    ")\n",
    "frb_data_clean = frb_data[features].fillna(0)\n",
    "scaler = StandardScaler()\n",
    "frb_data_scaled = scaler.fit_transform(frb_data_clean)\n",
    "\n",
    "# Retain the original indices\n",
    "indices = frb_data_clean.index\n",
    "\n",
    "# Split the data and retain indices\n",
    "train_data, val_data, train_labels, val_labels, train_indices, val_indices = train_test_split(\n",
    "    frb_data_scaled, labels, indices, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "val_tensor = torch.tensor(val_data, dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels.values, dtype=torch.long)\n",
    "val_labels_tensor = torch.tensor(val_labels.values, dtype=torch.long)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_tensor, train_labels_tensor)\n",
    "val_dataset = TensorDataset(val_tensor, val_labels_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "class SupervisedVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, dropout_rate=0.3, activation=nn.LeakyReLU(0.1)):\n",
    "        super(SupervisedVAE, self).__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # Additional dense layer\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),  # Additional dense layer\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "\n",
    "        # Classification head for binary classification - tune hyperparameters\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 2),\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),  # Added extra linear layer\n",
    "            self.activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 4, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        class_prob = self.classifier(mu)\n",
    "        return recon_x, mu, logvar, class_prob\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar, class_prob, labels, beta, gamma, class_weight, classification_multiplier):\n",
    "    reconstruction_loss_fn = nn.MSELoss(reduction='sum')\n",
    "    pos_weight = torch.tensor([class_weight], dtype=torch.float32, device=device)\n",
    "    classification_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight) # check this loss function\n",
    "    recon_loss = reconstruction_loss_fn(recon_x, x)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    class_loss = classification_multiplier * classification_loss_fn(class_prob, labels.unsqueeze(1).float())\n",
    "    total_loss = recon_loss + beta * kl_loss + gamma * class_loss\n",
    "    return total_loss, recon_loss, kl_loss, class_loss\n",
    "input_dim = val_tensor.shape[1]\n",
    "hidden_dim = 256\n",
    "latent_dim = 10\n",
    "stop_patience = 8\n",
    "num_epochs = 150\n",
    "def evaluate_classifier(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            class_logits = model(data)[-1]\n",
    "            preds = (class_logits > 0.5).float().cpu().numpy().squeeze()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, target_names=[\"Non-Repeater\", \"Repeater\"])\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    \n",
    "    false_positives = np.sum((all_labels == 0) & (all_preds == 1))\n",
    "\n",
    "    return accuracy, class_report, conf_matrix, recall, false_positives  # Return F1 score as well\n",
    "\n",
    "def get_activation_function(name):\n",
    "    if name == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    elif name == 'LeakyReLU':\n",
    "        return nn.LeakyReLU(0.1)\n",
    "    elif name == 'ELU':\n",
    "        return nn.ELU()\n",
    "    elif name == 'SELU':\n",
    "        return nn.SELU()\n",
    "    elif name == 'GELU':\n",
    "        return nn.GELU()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown activation function: {name}\")\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, f1_score\n",
    "import sklearn.metrics\n",
    "\n",
    "def evaluate_classifier_full(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data = data.to(device)\n",
    "            class_logits = model(data)[-1]\n",
    "            preds = (class_logits > 0.5).float().cpu().numpy().squeeze()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = sklearn.metrics.f1_score(all_labels, all_preds)\n",
    "    class_report = classification_report(all_labels, all_preds, target_names=[\"Non-Repeater\", \"Repeater\"])\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    return accuracy, class_report, conf_matrix, all_preds, all_labels\n",
    "\n",
    "original_data = pd.read_csv('frb-data.csv')\n",
    "original_data['Repeater'] = original_data['Repeater'].map({'Yes': 1, 'No': 0})\n",
    "print(original_data['Repeater'].isna().sum())\n",
    "\n",
    "print(f\"Number of NaN values in 'Repeater' column before processing: {original_data['Repeater'].isna().sum()}\")\n",
    "# Apply the function row-wise\n",
    "original_data['Repeater'] = original_data.apply(fill_repeater_from_source, axis=1, data=original_data)\n",
    "\n",
    "print(f\"Number of NaN values in 'Repeater' column after processing: {original_data['Repeater'].isna().sum()}\")\n",
    "best_params = {'hidden_dim': 1082, 'latent_dim': 18, 'beta': 1.149574612306723, 'gamma': 1.9210647260496314, 'dropout_rate': 0.13093239424733344, 'lr': 0.0011823749066137313, 'scheduler_patience': 7, 'class_weight': 0.35488674730648145, 'activation': 'ReLU', 'classification_multiplier': 7817.124805902009}\n",
    "\n",
    "beta = best_params[\"beta\"]\n",
    "gamma = best_params[\"gamma\"]\n",
    "lr = best_params[\"lr\"]\n",
    "scheduler_patience = best_params[\"scheduler_patience\"]\n",
    "num_epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e56a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_false_positives = \"\"\"FRB20181102A\n",
    "FRB20180309A\n",
    "FRB20141113A\n",
    "FRB20190221B\n",
    "FRB20210213A\n",
    "FRB20210303A\n",
    "FRB20200514B\n",
    "FRB20211212A\n",
    "FRB20220506D\n",
    "FRB20150418A\n",
    "FRB20190423B\n",
    "FRB20010621A\n",
    "FRB20190429B\n",
    "FRB20010125A\n",
    "FRB20191109A\n",
    "FRB20190625A\n",
    "FRB20191020B\n",
    "FRB20220725A\n",
    "FRB20210408H\n",
    "FRB20190420A\n",
    "FRB20180907E\n",
    "FRB20140514A\n",
    "FRB20010305A\n",
    "FRB20110523A\n",
    "FRB20010312A\n",
    "FRB20190714A\n",
    "FRB20191221A\n",
    "FRB20210206A\n",
    "FRB20221101A\n",
    "FRB20230718A\n",
    "FRB20190112A\n",
    "FRB20200917A\n",
    "FRB20200125A\n",
    "FRB20200405A\n",
    "FRB20210202D\"\"\".split('\\n')\n",
    "all_false_positives = [source.strip() for source in all_false_positives if source.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697ff242",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_latent_representations(model, dataloader, device):\n",
    "    model.eval()\n",
    "    latent_representations = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data = data.to(device)\n",
    "            mu, _ = model.encode(data)\n",
    "            latent_representations.append(mu.cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "    return np.concatenate(latent_representations), np.concatenate(all_labels)\n",
    "\n",
    "\n",
    "def natural_keys(text):\n",
    "    \"\"\"Helper to sort text alphanumerically.\"\"\"\n",
    "    return [int(c) if c.isdigit() else c.lower() for c in re.split(r'(\\d+)', text)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17d75b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Evaluation ===\n",
      "Validation Accuracy: 0.9176\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Repeater       0.91      0.94      0.92       150\n",
      "    Repeater       0.93      0.89      0.91       129\n",
      "\n",
      "    accuracy                           0.92       279\n",
      "   macro avg       0.92      0.92      0.92       279\n",
      "weighted avg       0.92      0.92      0.92       279\n",
      "\n",
      "Confusion Matrix:\n",
      " [[141   9]\n",
      " [ 14 115]]\n"
     ]
    }
   ],
   "source": [
    "#import DBSCAN\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Step 1: Identify the 5 most frequent repeaters\n",
    "repeater_counts = frb_data[frb_data['Repeater'] == 1]['Source'].value_counts()\n",
    "top_5_repeaters = repeater_counts.head(5).index.tolist()\n",
    "\n",
    "# # Step 2: Filter the dataset to exclude these repeaters\n",
    "# filtered_indices = frb_data[~frb_data['Source'].isin(top_5_repeaters)].index\n",
    "# filtered_data_clean = frb_data_clean.loc[filtered_indices]\n",
    "# filtered_labels = labels.loc[filtered_indices]\n",
    "\n",
    "top_5_repeaters = frb_data[frb_data['Repeater'] == 1]['Source'].value_counts().head(5).index.tolist()\n",
    "\n",
    "# Sample 10 signals from each top repeater\n",
    "sampled_indices = []\n",
    "for repeater in top_5_repeaters:\n",
    "    repeater_indices = frb_data[frb_data['Source'] == repeater].index\n",
    "    sampled = np.random.choice(repeater_indices, size=min(5, len(repeater_indices)), replace=False)\n",
    "    sampled_indices.extend(sampled)\n",
    "\n",
    "# Get all other indices excluding the top 5 repeaters\n",
    "non_top5_indices = frb_data[~frb_data['Source'].isin(top_5_repeaters)].index\n",
    "\n",
    "# Combine the sampled top5 and the rest\n",
    "final_indices = np.concatenate([non_top5_indices, sampled_indices])\n",
    "\n",
    "# Filter the clean data and labels accordingly\n",
    "filtered_data_clean = frb_data_clean.loc[final_indices]\n",
    "filtered_labels = labels.loc[final_indices]\n",
    "\n",
    "\n",
    "# Step 3: Standardize the filtered dataset\n",
    "scaler = StandardScaler()\n",
    "filtered_data_scaled = scaler.fit_transform(filtered_data_clean)\n",
    "\n",
    "# Step 4: Split the filtered dataset into train and validation sets\n",
    "train_data_filtered, val_data_filtered, train_labels_filtered, val_labels_filtered = train_test_split(\n",
    "    filtered_data_scaled, filtered_labels, test_size=0.2, random_state=42, stratify=filtered_labels\n",
    ")\n",
    "\n",
    "# Step 5: Convert to PyTorch tensors\n",
    "train_tensor_filtered = torch.tensor(train_data_filtered, dtype=torch.float32)\n",
    "val_tensor_filtered = torch.tensor(val_data_filtered, dtype=torch.float32)\n",
    "train_labels_tensor_filtered = torch.tensor(train_labels_filtered.values, dtype=torch.long)\n",
    "val_labels_tensor_filtered = torch.tensor(val_labels_filtered.values, dtype=torch.long)\n",
    "\n",
    "# Step 6: Create datasets and dataloaders\n",
    "batch_size = 64\n",
    "train_dataset_filtered = TensorDataset(train_tensor_filtered, train_labels_tensor_filtered)\n",
    "val_dataset_filtered = TensorDataset(val_tensor_filtered, val_labels_tensor_filtered)\n",
    "\n",
    "train_loader_filtered = DataLoader(train_dataset_filtered, batch_size=batch_size, shuffle=True)\n",
    "val_loader_filtered = DataLoader(val_dataset_filtered, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "def train_supervised_filtered(model, optimizer, scheduler, epoch, beta, gamma, class_weight, classification_multiplier):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    recon_loss_total = 0\n",
    "    kl_loss_total = 0\n",
    "    classification_loss_total = 0\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, labels) in enumerate(train_loader_filtered):\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar, class_logits = model(data)\n",
    "        \n",
    "        # Supervised loss function\n",
    "        loss, recon_loss, kl_loss, classification_loss = loss_function(\n",
    "            recon_batch, data, mu, logvar, class_logits, labels, beta, gamma, class_weight, classification_multiplier\n",
    "        )\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        recon_loss_total += recon_loss.item()\n",
    "        kl_loss_total += kl_loss.item()\n",
    "        classification_loss_total += classification_loss.item()\n",
    "        \n",
    "        predicted = (class_logits > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.squeeze() == labels).sum().item()\n",
    "    \n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_recon = recon_loss_total / len(train_loader.dataset)\n",
    "    avg_kl = kl_loss_total / len(train_loader.dataset)\n",
    "    avg_class = classification_loss_total / len(train_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # print(f'====> Epoch: {epoch} Average loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}, '\n",
    "    #       f'Class: {avg_class:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    return avg_loss, avg_recon, avg_kl, avg_class, accuracy\n",
    "\n",
    "def validate_supervised_filtered(model, scheduler, optimizer, epoch, beta, gamma, class_weight, classification_multiplier):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    recon_loss_total = 0\n",
    "    kl_loss_total = 0\n",
    "    classification_loss_total = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in val_loader_filtered:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            recon_batch, mu, logvar, class_logits = model(data)\n",
    "            \n",
    "            loss, recon_loss, kl_loss, classification_loss = loss_function(\n",
    "                recon_batch, data, mu, logvar, class_logits, labels, beta, gamma, class_weight, classification_multiplier\n",
    "            )\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            recon_loss_total += recon_loss.item()\n",
    "            kl_loss_total += kl_loss.item()\n",
    "            classification_loss_total += classification_loss.item()\n",
    "            \n",
    "            predicted = (class_logits > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.squeeze() == labels).sum().item()\n",
    "    \n",
    "    avg_loss = val_loss / len(val_loader.dataset)\n",
    "    avg_recon = recon_loss_total / len(val_loader.dataset)\n",
    "    avg_kl = kl_loss_total / len(val_loader.dataset)\n",
    "    avg_class = classification_loss_total / len(val_loader.dataset)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # print(f'====> Validation loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}, '\n",
    "    #       f'Class: {avg_class:.4f}, Accuracy: {accuracy:.4f}')\n",
    "    return avg_loss, avg_recon, avg_kl, avg_class, accuracy\n",
    "\n",
    "\n",
    "def early_stopping(val_losses, patience):\n",
    "    if len(val_losses) > patience:\n",
    "        if all(val_losses[-i-1] <= val_losses[-i] for i in range(1, patience+1)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# Step 7: Train the model with the filtered dataset\n",
    "best_model = SupervisedVAE(\n",
    "    input_dim, \n",
    "    best_params[\"hidden_dim\"], \n",
    "    best_params[\"latent_dim\"], \n",
    "    best_params[\"dropout_rate\"], \n",
    "    activation=get_activation_function(best_params[\"activation\"])\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params[\"lr\"])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=best_params[\"scheduler_patience\"])\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, _, _, _, train_accuracy = train_supervised_filtered(\n",
    "        best_model, optimizer, scheduler, epoch, best_params[\"beta\"], \n",
    "        best_params[\"gamma\"], best_params[\"class_weight\"], best_params[\"classification_multiplier\"]\n",
    "    )\n",
    "    \n",
    "    val_loss, _, _, _, val_accuracy = validate_supervised_filtered(\n",
    "        best_model, optimizer, scheduler, epoch, best_params[\"beta\"], \n",
    "        best_params[\"gamma\"], best_params[\"class_weight\"], best_params[\"classification_multiplier\"]\n",
    "    )\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if early_stopping([val_loss], stop_patience):\n",
    "        print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "# Evaluate the model after training\n",
    "val_accuracy, val_class_report, val_conf_matrix, val_preds, val_labels = evaluate_classifier_full(best_model, val_loader_filtered, device)\n",
    "\n",
    "print(\"\\n=== Final Evaluation ===\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Classification Report:\\n\", val_class_report)\n",
    "print(\"Confusion Matrix:\\n\", val_conf_matrix)\n",
    "# Step 1: Extract latent representations\n",
    "train_latent, train_labels_np = get_latent_representations(best_model, train_loader_filtered, device)\n",
    "val_latent, val_labels_np = get_latent_representations(best_model, val_loader_filtered, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ac62acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRB20150418A is similar to FRB20220912A with a score of 3.07404 (Repeater difference)\n",
      "FRB20150418A is similar to FRB20190520B with a score of 6.70921 (Repeater difference)\n",
      "FRB20190112A is similar to FRB20191106C with a score of 0.84459 (Repeater difference)\n",
      "FRB20190112A is similar to FRB20190915D with a score of 1.01321 (Repeater difference)\n",
      "FRB20190112A is similar to FRB20190328C with a score of 1.23229 (Repeater difference)\n",
      "FRB20200917A is similar to FRB20200926A with a score of 0.86790 (Repeater difference)\n",
      "FRB20200917A is similar to FRB20190208A with a score of 0.68648 (Repeater difference)\n",
      "FRB20200917A is similar to FRB20190127B with a score of 0.73771 (Repeater difference)\n",
      "FRB20200917A is similar to FRB20181128A with a score of 0.92494 (Repeater difference)\n",
      "FRB20200125A is similar to FRB20180814A with a score of 0.15985 (Repeater difference)\n",
      "FRB20200125A is similar to FRB20190303A with a score of 0.14750 (Repeater difference)\n",
      "FRB20200125A is similar to FRB20200223B with a score of 0.15475 (Repeater difference)\n"
     ]
    }
   ],
   "source": [
    "val_index = val_labels_filtered.index\n",
    "val_sources = original_data.loc[val_index, 'Source'].values\n",
    "\n",
    "latent_df = pd.DataFrame(val_latent, index=val_index)\n",
    "latent_df['Source'] = val_sources\n",
    "\n",
    "# Fit Nearest Neighbors\n",
    "nbrs = NearestNeighbors(n_neighbors=6).fit(val_latent)\n",
    "distances, indices = nbrs.kneighbors(val_latent)\n",
    "\n",
    "# Simple nearest neighbor lookup (no cluster checks)\n",
    "def find_similar(source):\n",
    "    idx = latent_df[latent_df['Source'] == source].index[0]\n",
    "    pos = list(latent_df.index).index(idx)\n",
    "\n",
    "    neighbors = {}\n",
    "    for i, neighbor_pos in enumerate(indices[pos][1:]):  # skip self (first neighbor)\n",
    "        neighbor_idx = latent_df.index[neighbor_pos]\n",
    "        neighbors[latent_df.loc[neighbor_idx, 'Source']] = float(distances[pos][i+1])\n",
    "    return neighbors\n",
    "\n",
    "# Track pairs where a non-repeater is similar to a repeater\n",
    "highlight_non_repeaters = set()\n",
    "highlight_repeaters = set()\n",
    "\n",
    "for source in all_false_positives:\n",
    "    # if source in garcia_list and source in zhu_ge_list and source in luo_list and source in latent_df['Source'].values:\n",
    "    if source in latent_df['Source'].values:\n",
    "\n",
    "        similar_source_data = find_similar(source)\n",
    "        if similar_source_data:\n",
    "            for similar_source, score in similar_source_data.items():\n",
    "                source_repeater = frb_data[frb_data['Source'] == source]['Repeater'].values[0]\n",
    "                neighbor_repeater = frb_data[frb_data['Source'] == similar_source]['Repeater'].values[0]\n",
    "\n",
    "                if source_repeater != neighbor_repeater:\n",
    "                    print(f\"{source} is similar to {similar_source} with a score of {score:.5f} (Repeater difference)\")\n",
    "                    if source_repeater == 0:\n",
    "                        highlight_non_repeaters.add(source)\n",
    "                        highlight_repeaters.add(similar_source)\n",
    "                    else:\n",
    "                        highlight_non_repeaters.add(similar_source)\n",
    "                        highlight_repeaters.add(source)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".frb-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
