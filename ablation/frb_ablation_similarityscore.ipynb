{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7259e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablating feature: Observing_band\n",
      "Validation Accuracy after ablating Observing_band: 0.8996\n",
      "Confusion Matrix:\n",
      "[[141   9]\n",
      " [ 19 110]]\n",
      "Ablating feature: SNR\n",
      "Validation Accuracy after ablating SNR: 0.9104\n",
      "Confusion Matrix:\n",
      "[[138  12]\n",
      " [ 13 116]]\n",
      "Ablating feature: Freq_high\n",
      "Validation Accuracy after ablating Freq_high: 0.8459\n",
      "Confusion Matrix:\n",
      "[[121  29]\n",
      " [ 14 115]]\n",
      "Ablating feature: Freq_low\n",
      "Validation Accuracy after ablating Freq_low: 0.8889\n",
      "Confusion Matrix:\n",
      "[[133  17]\n",
      " [ 14 115]]\n",
      "Ablating feature: Freq_peak\n",
      "Validation Accuracy after ablating Freq_peak: 0.8315\n",
      "Confusion Matrix:\n",
      "[[112  38]\n",
      " [  9 120]]\n",
      "Ablating feature: Width\n",
      "Validation Accuracy after ablating Width: 0.8853\n",
      "Confusion Matrix:\n",
      "[[144   6]\n",
      " [ 26 103]]\n",
      "Ablating feature: DM_SNR\n",
      "Validation Accuracy after ablating DM_SNR: 0.8996\n",
      "Confusion Matrix:\n",
      "[[141   9]\n",
      " [ 19 110]]\n",
      "Ablating feature: DM_alig\n",
      "Validation Accuracy after ablating DM_alig: 0.8853\n",
      "Confusion Matrix:\n",
      "[[143   7]\n",
      " [ 25 104]]\n",
      "Ablating feature: Flux_density\n",
      "Validation Accuracy after ablating Flux_density: 0.8996\n",
      "Confusion Matrix:\n",
      "[[137  13]\n",
      " [ 15 114]]\n",
      "Ablating feature: Fluence\n",
      "Validation Accuracy after ablating Fluence: 0.9104\n",
      "Confusion Matrix:\n",
      "[[141   9]\n",
      " [ 16 113]]\n",
      "Ablating feature: Energy\n",
      "Validation Accuracy after ablating Energy: 0.9104\n",
      "Confusion Matrix:\n",
      "[[139  11]\n",
      " [ 14 115]]\n",
      "Ablating feature: Polar_l\n",
      "Validation Accuracy after ablating Polar_l: 0.8925\n",
      "Confusion Matrix:\n",
      "[[142   8]\n",
      " [ 22 107]]\n",
      "Ablating feature: Polar_c\n",
      "Validation Accuracy after ablating Polar_c: 0.9068\n",
      "Confusion Matrix:\n",
      "[[142   8]\n",
      " [ 18 111]]\n",
      "Ablating feature: RM_syn\n",
      "Validation Accuracy after ablating RM_syn: 0.8136\n",
      "Confusion Matrix:\n",
      "[[111  39]\n",
      " [ 13 116]]\n",
      "Ablating feature: RM_QUfit\n",
      "Validation Accuracy after ablating RM_QUfit: 0.9104\n",
      "Confusion Matrix:\n",
      "[[139  11]\n",
      " [ 14 115]]\n",
      "Ablating feature: Scatt_t\n",
      "Validation Accuracy after ablating Scatt_t: 0.8817\n",
      "Confusion Matrix:\n",
      "[[145   5]\n",
      " [ 28 101]]\n",
      "Ablating feature: DM_SNR_upper\n",
      "Validation Accuracy after ablating DM_SNR_upper: 0.8817\n",
      "Confusion Matrix:\n",
      "[[129  21]\n",
      " [ 12 117]]\n",
      "Ablating feature: DM_alig_upper\n",
      "Validation Accuracy after ablating DM_alig_upper: 0.9211\n",
      "Confusion Matrix:\n",
      "[[141   9]\n",
      " [ 13 116]]\n",
      "Ablating feature: Flux_density_upper\n",
      "Validation Accuracy after ablating Flux_density_upper: 0.9104\n",
      "Confusion Matrix:\n",
      "[[140  10]\n",
      " [ 15 114]]\n",
      "Ablating feature: Fluence_upper\n",
      "Validation Accuracy after ablating Fluence_upper: 0.9176\n",
      "Confusion Matrix:\n",
      "[[141   9]\n",
      " [ 14 115]]\n",
      "Ablating feature: Energy_upper\n",
      "Validation Accuracy after ablating Energy_upper: 0.9068\n",
      "Confusion Matrix:\n",
      "[[139  11]\n",
      " [ 15 114]]\n",
      "Ablating feature: Polar_l_upper\n",
      "Validation Accuracy after ablating Polar_l_upper: 0.9140\n",
      "Confusion Matrix:\n",
      "[[140  10]\n",
      " [ 14 115]]\n",
      "Ablating feature: Polar_c_upper\n",
      "Validation Accuracy after ablating Polar_c_upper: 0.8710\n",
      "Confusion Matrix:\n",
      "[[125  25]\n",
      " [ 11 118]]\n",
      "Ablating feature: RM_syn_upper\n",
      "Validation Accuracy after ablating RM_syn_upper: 0.8925\n",
      "Confusion Matrix:\n",
      "[[141   9]\n",
      " [ 21 108]]\n",
      "Ablating feature: RM_QUfit_upper\n",
      "Validation Accuracy after ablating RM_QUfit_upper: 0.9104\n",
      "Confusion Matrix:\n",
      "[[141   9]\n",
      " [ 16 113]]\n",
      "Ablating feature: Scatt_t_upper\n",
      "Validation Accuracy after ablating Scatt_t_upper: 0.9319\n",
      "Confusion Matrix:\n",
      "[[144   6]\n",
      " [ 13 116]]\n",
      "Ablating feature: DM_SNR_lower\n",
      "Validation Accuracy after ablating DM_SNR_lower: 0.8996\n",
      "Confusion Matrix:\n",
      "[[140  10]\n",
      " [ 18 111]]\n",
      "Ablating feature: DM_alig_lower\n",
      "Validation Accuracy after ablating DM_alig_lower: 0.8889\n",
      "Confusion Matrix:\n",
      "[[141   9]\n",
      " [ 22 107]]\n",
      "Ablating feature: Flux_density_lower\n",
      "Validation Accuracy after ablating Flux_density_lower: 0.9176\n",
      "Confusion Matrix:\n",
      "[[139  11]\n",
      " [ 12 117]]\n",
      "Ablating feature: Fluence_lower\n",
      "Validation Accuracy after ablating Fluence_lower: 0.9068\n",
      "Confusion Matrix:\n",
      "[[142   8]\n",
      " [ 18 111]]\n",
      "Ablating feature: Energy_lower\n",
      "Validation Accuracy after ablating Energy_lower: 0.8996\n",
      "Confusion Matrix:\n",
      "[[141   9]\n",
      " [ 19 110]]\n",
      "Ablating feature: Polar_l_lower\n",
      "Validation Accuracy after ablating Polar_l_lower: 0.9032\n",
      "Confusion Matrix:\n",
      "[[140  10]\n",
      " [ 17 112]]\n",
      "Ablating feature: Polar_c_lower\n",
      "Validation Accuracy after ablating Polar_c_lower: 0.9104\n",
      "Confusion Matrix:\n",
      "[[138  12]\n",
      " [ 13 116]]\n",
      "Ablating feature: RM_syn_lower\n",
      "Validation Accuracy after ablating RM_syn_lower: 0.8889\n",
      "Confusion Matrix:\n",
      "[[134  16]\n",
      " [ 15 114]]\n",
      "Ablating feature: RM_QUfit_lower\n",
      "Validation Accuracy after ablating RM_QUfit_lower: 0.9068\n",
      "Confusion Matrix:\n",
      "[[142   8]\n",
      " [ 18 111]]\n",
      "Ablating feature: Scatt_t_lower\n",
      "Validation Accuracy after ablating Scatt_t_lower: 0.9032\n",
      "Confusion Matrix:\n",
      "[[144   6]\n",
      " [ 21 108]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# set device and seeds\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# handle special source mapping\n",
    "def fill_repeater_from_source(row):\n",
    "    if row['Source'] == 'FRB20220912A':\n",
    "        return 1\n",
    "    return row['Repeater']\n",
    "\n",
    "# clean numeric entries\n",
    "def clean_numeric_value(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        if not value:\n",
    "            return np.nan\n",
    "        try:\n",
    "            for char in ['/', '+', '<', '>', '~']:\n",
    "                value = value.replace(char, '')\n",
    "            if '-' in value:\n",
    "                value = value.split('-')[0]\n",
    "            return float(value)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "# choose activation function\n",
    "def get_activation_function(name):\n",
    "    if name == 'ReLU':\n",
    "        return nn.ReLU()\n",
    "    if name == 'LeakyReLU':\n",
    "        return nn.LeakyReLU(0.1)\n",
    "    if name == 'ELU':\n",
    "        return nn.ELU()\n",
    "    if name == 'SELU':\n",
    "        return nn.SELU()\n",
    "    if name == 'GELU':\n",
    "        return nn.GELU()\n",
    "    raise ValueError(f\"Unknown activation: {name}\")\n",
    "\n",
    "# define supervised VAE\n",
    "class SupervisedVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, dropout_rate=0.3, activation=nn.LeakyReLU(0.1)):\n",
    "        super(SupervisedVAE, self).__init__()\n",
    "        self.activation = activation\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            activation,\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim // 2),\n",
    "            activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            activation,\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim // 4, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        class_prob = self.classifier(mu)\n",
    "        return recon_x, mu, logvar, class_prob\n",
    "\n",
    "# loss for VAE and classification\n",
    "def loss_function(recon_x, x, mu, logvar, class_prob, labels, beta, gamma, class_weight, classification_multiplier):\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    pos_weight = torch.tensor([class_weight], dtype=torch.float32, device=device)\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    recon_loss = mse(recon_x, x)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    class_loss = classification_multiplier * bce(class_prob, labels.unsqueeze(1).float())\n",
    "    total = recon_loss + beta * kl_loss + gamma * class_loss\n",
    "    return total, recon_loss, kl_loss, class_loss\n",
    "\n",
    "# train for one epoch\n",
    "def train_epoch(model, loader, optimizer, beta, gamma, class_weight, classification_multiplier):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    for data, labels in loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar, logits = model(data)\n",
    "        loss, *_ = loss_function(recon, data, mu, logvar, logits, labels, beta, gamma, class_weight, classification_multiplier)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.item()\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "# validate for one epoch\n",
    "def validate_epoch(model, loader, beta, gamma, class_weight, classification_multiplier):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            recon, mu, logvar, logits = model(data)\n",
    "            loss, *_ = loss_function(recon, data, mu, logvar, logits, labels, beta, gamma, class_weight, classification_multiplier)\n",
    "            total += loss.item()\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "# stop if no improvement\n",
    "def early_stopping(losses, patience):\n",
    "    if len(losses) <= patience:\n",
    "        return False\n",
    "    return all(losses[-i-2] <= losses[-i-1] for i in range(1, patience+1))\n",
    "\n",
    "# get latent codes\n",
    "def get_latent_representations(model, loader):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, lbl in loader:\n",
    "            data = data.to(device)\n",
    "            mu, _ = model.encode(data)\n",
    "            latents.append(mu.cpu().numpy())\n",
    "            labels.append(lbl.numpy())\n",
    "    return np.concatenate(latents), np.concatenate(labels)\n",
    "\n",
    "# evaluate classification\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            logits = model(data)[-1]\n",
    "            preds = (logits > 0.5).float().cpu().numpy().squeeze()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"Non-Repeater\", \"Repeater\"])\n",
    "    matrix = confusion_matrix(all_labels, all_preds)\n",
    "    return accuracy, report, matrix\n",
    "\n",
    "# load and clean data\n",
    "frb_data = pd.read_csv('frb-data.csv')\n",
    "frb_data['Repeater'] = frb_data['Repeater'].map({'Yes': 1, 'No': 0}).fillna(0).astype(int)\n",
    "frb_data['Repeater'] = frb_data.apply(fill_repeater_from_source, axis=1)\n",
    "error_features = ['DM_SNR', 'DM_alig', 'Flux_density', 'Fluence', 'Energy', 'Polar_l', 'Polar_c', 'RM_syn', 'RM_QUfit', 'Scatt_t']\n",
    "base_features = ['Observing_band', 'SNR', 'Freq_high', 'Freq_low', 'Freq_peak', 'Width']\n",
    "for feat in base_features + error_features:\n",
    "    frb_data[feat] = frb_data[feat].apply(clean_numeric_value)\n",
    "    frb_data[f'{feat}_err'] = frb_data.get(f'{feat}_err', pd.Series(np.nan)).apply(clean_numeric_value)\n",
    "for feat in error_features:\n",
    "    frb_data[f'{feat}_upper'] = frb_data[feat] + frb_data[f'{feat}_err']\n",
    "    frb_data[f'{feat}_lower'] = (frb_data[feat] - frb_data[f'{feat}_err']).clip(lower=0)\n",
    "features = base_features + error_features + [f'{f}_upper' for f in error_features] + [f'{f}_lower' for f in error_features]\n",
    "frb_data_clean = frb_data[features].fillna(0)\n",
    "labels = frb_data['Repeater']\n",
    "original_data = frb_data.copy()\n",
    "all_false_positives = ['FRB20181102A','FRB20180309A','FRB20141113A','FRB20190221B','FRB20210213A','FRB20210303A','FRB20200514B','FRB20211212A','FRB20220506D','FRB20150418A','FRB20190423B','FRB20010621A','FRB20190429B','FRB20010125A','FRB20191109A','FRB20190625A','FRB20191020B','FRB20220725A','FRB20210408H','FRB20190420A','FRB20180907E','FRB20140514A','FRB20010305A','FRB20110523A','FRB20010312A','FRB20190714A','FRB20191221A','FRB20210206A','FRB20221101A','FRB20230718A','FRB20190112A','FRB20200917A','FRB20200125A','FRB20200405A','FRB20210202D']\n",
    "\n",
    "# set hyperparameters\n",
    "best_params = {'hidden_dim':1082,'latent_dim':18,'beta':1.149574612306723,'gamma':1.9210647260496314,'dropout_rate':0.13093239424733344,'lr':0.0011823749066137313,'scheduler_patience':7,'class_weight':0.35488674730648145,'classification_multiplier':7817.124805902009,'activation':'ReLU'}\n",
    "stop_patience = 8\n",
    "num_epochs = 100\n",
    "\n",
    "# ablation loop\n",
    "for feature in features:\n",
    "    print(f\"Ablating feature: {feature}\")\n",
    "    feats = [f for f in features if f != feature]\n",
    "    subset = frb_data_clean[feats]\n",
    "    repeater_counts = frb_data[frb_data['Repeater']==1]['Source'].value_counts()\n",
    "    top5 = repeater_counts.head(5).index.tolist()\n",
    "    sampled = []\n",
    "    for rep in top5:\n",
    "        idxs = frb_data[frb_data['Source']==rep].index\n",
    "        sampled.extend(np.random.choice(idxs, size=min(5,len(idxs)), replace=False))\n",
    "    non_top5 = frb_data[~frb_data['Source'].isin(top5)].index\n",
    "    final_idx = np.concatenate([non_top5, sampled])\n",
    "    data_ab = subset.loc[final_idx]\n",
    "    labels_ab = labels.loc[final_idx]\n",
    "    scaled = StandardScaler().fit_transform(data_ab)\n",
    "    train_x, val_x, train_y, val_y = train_test_split(scaled, labels_ab, test_size=0.2, random_state=42, stratify=labels_ab)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(train_x,dtype=torch.float32), torch.tensor(train_y.values,dtype=torch.long)), batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(torch.tensor(val_x,dtype=torch.float32), torch.tensor(val_y.values,dtype=torch.long)), batch_size=64, shuffle=False)\n",
    "\n",
    "    model = SupervisedVAE(input_dim=val_x.shape[1], hidden_dim=best_params['hidden_dim'], latent_dim=best_params['latent_dim'], dropout_rate=best_params['dropout_rate'], activation=get_activation_function(best_params['activation'])).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=best_params['scheduler_patience'])\n",
    "\n",
    "    val_losses = []\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        train_epoch(model, train_loader, optimizer, best_params['beta'], best_params['gamma'], best_params['class_weight'], best_params['classification_multiplier'])\n",
    "        vloss = validate_epoch(model, val_loader, best_params['beta'], best_params['gamma'], best_params['class_weight'], best_params['classification_multiplier'])\n",
    "        val_losses.append(vloss)\n",
    "        scheduler.step(vloss)\n",
    "        if early_stopping(val_losses, stop_patience):\n",
    "            break\n",
    "\n",
    "    # classification performance\n",
    "    acc, report, cm = evaluate(model, val_loader)\n",
    "    print(f\"Validation Accuracy after ablating {feature}: {acc:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # nearest neighbor similarity\n",
    "    val_latent, _ = get_latent_representations(model, val_loader)\n",
    "    val_idx = val_y.index\n",
    "    latent_df = pd.DataFrame(val_latent, index=val_idx)\n",
    "    latent_df['Source'] = original_data.loc[val_idx,'Source'].values\n",
    "    nbrs = NearestNeighbors(n_neighbors=6).fit(val_latent)\n",
    "    dists, inds = nbrs.kneighbors(val_latent)\n",
    "    results = []\n",
    "    for src in all_false_positives:\n",
    "        if src in latent_df['Source'].values:\n",
    "            idx0 = latent_df[latent_df['Source']==src].index[0]\n",
    "            pos = latent_df.index.get_loc(idx0)\n",
    "            for npos, dist in zip(inds[pos][1:], dists[pos][1:]):\n",
    "                nbr_idx = latent_df.index[npos]\n",
    "                nbr_src = latent_df.loc[nbr_idx,'Source']\n",
    "                if labels.loc[idx0] != labels.loc[nbr_idx]:\n",
    "                    if labels.loc[idx0] == 0:\n",
    "                        non_rep, rep = src, nbr_src\n",
    "                    else:\n",
    "                        non_rep, rep = nbr_src, src\n",
    "                    results.append((non_rep, rep, float(dist)))\n",
    "    df_out = pd.DataFrame(results, columns=['Non-Repeater','Repeater','Score'])\n",
    "    df_out.to_csv(f\"ablated_results/{feature}_similar_signals.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".frb-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
