{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "534ceecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=33018.1137, val_loss=29742.3203, train_acc=0.6399, val_acc=0.5597\n",
      "Epoch 2: train_loss=22320.3955, val_loss=42907.5988, train_acc=0.5724, val_acc=0.5932\n",
      "Epoch 3: train_loss=18624.5551, val_loss=20186.4049, train_acc=0.6109, val_acc=0.5817\n",
      "Epoch 4: train_loss=19623.9426, val_loss=21332.9525, train_acc=0.6319, val_acc=0.6024\n",
      "Epoch 5: train_loss=19140.2292, val_loss=21948.5171, train_acc=0.6058, val_acc=0.6047\n",
      "Epoch 6: train_loss=18674.4522, val_loss=20021.0766, train_acc=0.6923, val_acc=0.6884\n",
      "Epoch 7: train_loss=17781.6923, val_loss=19549.8854, train_acc=0.6759, val_acc=0.6601\n",
      "Epoch 8: train_loss=15081.0493, val_loss=14942.9997, train_acc=0.6311, val_acc=0.6411\n",
      "Epoch 9: train_loss=15705.2692, val_loss=22289.2445, train_acc=0.6899, val_acc=0.6405\n",
      "Epoch 10: train_loss=15976.8720, val_loss=17336.4876, train_acc=0.6996, val_acc=0.7467\n",
      "Epoch 11: train_loss=14693.3480, val_loss=18692.4202, train_acc=0.6980, val_acc=0.7328\n",
      "Epoch 12: train_loss=14187.6983, val_loss=16785.5578, train_acc=0.6860, val_acc=0.7219\n",
      "Epoch 13: train_loss=12139.8098, val_loss=14897.2013, train_acc=0.6794, val_acc=0.7034\n",
      "Epoch 14: train_loss=12312.1730, val_loss=16755.1605, train_acc=0.6820, val_acc=0.6901\n",
      "Epoch 15: train_loss=11769.8324, val_loss=21442.1082, train_acc=0.6468, val_acc=0.6722\n",
      "Epoch 16: train_loss=18761.3630, val_loss=25360.0508, train_acc=0.6279, val_acc=0.6018\n",
      "Epoch 17: train_loss=15500.7441, val_loss=21010.3517, train_acc=0.5545, val_acc=0.6093\n",
      "Epoch 18: train_loss=11852.2667, val_loss=22031.2932, train_acc=0.5966, val_acc=0.6180\n",
      "Epoch 19: train_loss=11629.6306, val_loss=22868.4454, train_acc=0.6089, val_acc=0.5892\n",
      "Epoch 20: train_loss=10733.7981, val_loss=18864.8755, train_acc=0.5898, val_acc=0.6561\n",
      "Epoch 21: train_loss=13007.0020, val_loss=18447.9039, train_acc=0.5969, val_acc=0.6070\n",
      "Epoch 22: train_loss=10821.5806, val_loss=19536.4710, train_acc=0.6188, val_acc=0.6261\n",
      "Epoch 23: train_loss=8911.3661, val_loss=16116.9554, train_acc=0.6358, val_acc=0.6070\n",
      "Epoch 24: train_loss=8183.9390, val_loss=16468.7316, train_acc=0.6356, val_acc=0.6151\n",
      "Epoch 25: train_loss=8713.4625, val_loss=17840.8346, train_acc=0.6234, val_acc=0.6780\n",
      "Epoch 26: train_loss=8963.7365, val_loss=34726.6920, train_acc=0.6697, val_acc=0.7409\n",
      "Epoch 27: train_loss=10671.1018, val_loss=14662.1675, train_acc=0.6761, val_acc=0.6694\n",
      "Epoch 28: train_loss=8941.3588, val_loss=85606.0449, train_acc=0.6327, val_acc=0.6065\n",
      "Epoch 29: train_loss=8663.7251, val_loss=15877.3508, train_acc=0.6164, val_acc=0.4737\n",
      "Epoch 30: train_loss=7702.8116, val_loss=18074.7023, train_acc=0.5878, val_acc=0.6267\n",
      "Epoch 31: train_loss=9481.2820, val_loss=21703.1108, train_acc=0.6361, val_acc=0.6411\n",
      "Epoch 32: train_loss=9336.2893, val_loss=16458.7470, train_acc=0.6448, val_acc=0.6711\n",
      "Epoch 33: train_loss=8812.5458, val_loss=16283.2223, train_acc=0.6656, val_acc=0.6867\n",
      "Epoch 34: train_loss=9105.6917, val_loss=14962.3292, train_acc=0.6904, val_acc=0.7201\n",
      "Epoch 35: train_loss=7159.7061, val_loss=14658.8871, train_acc=0.6973, val_acc=0.6353\n",
      "Epoch 36: train_loss=7818.4510, val_loss=16199.9620, train_acc=0.6905, val_acc=0.6826\n",
      "Epoch 37: train_loss=8523.4309, val_loss=18005.3219, train_acc=0.6166, val_acc=0.5765\n",
      "Epoch 38: train_loss=7540.1223, val_loss=16201.7168, train_acc=0.5960, val_acc=0.6238\n",
      "Epoch 39: train_loss=7890.4253, val_loss=14163.9412, train_acc=0.5983, val_acc=0.6226\n",
      "Epoch 40: train_loss=6951.9463, val_loss=20622.1926, train_acc=0.6104, val_acc=0.6232\n",
      "Epoch 41: train_loss=7508.3461, val_loss=21830.9175, train_acc=0.5983, val_acc=0.6497\n",
      "Epoch 42: train_loss=6933.9084, val_loss=17857.5479, train_acc=0.6276, val_acc=0.6422\n",
      "Epoch 43: train_loss=6892.1522, val_loss=22298.6261, train_acc=0.6022, val_acc=0.6001\n",
      "Epoch 44: train_loss=7636.6765, val_loss=18189.8938, train_acc=0.6120, val_acc=0.5793\n",
      "Epoch 45: train_loss=7468.8059, val_loss=15171.0554, train_acc=0.6138, val_acc=0.6324\n",
      "Epoch 46: train_loss=7830.4179, val_loss=18660.3051, train_acc=0.6256, val_acc=0.6711\n",
      "Epoch 47: train_loss=9250.2970, val_loss=19312.0682, train_acc=0.6530, val_acc=0.6567\n",
      "Epoch 48: train_loss=6316.0330, val_loss=16602.3313, train_acc=0.6553, val_acc=0.6532\n",
      "Epoch 49: train_loss=6132.1719, val_loss=16500.7399, train_acc=0.6425, val_acc=0.6746\n",
      "Epoch 50: train_loss=5467.3759, val_loss=16601.0121, train_acc=0.6484, val_acc=0.6607\n",
      "Epoch 51: train_loss=5484.7154, val_loss=20175.1612, train_acc=0.6357, val_acc=0.6469\n",
      "Epoch 52: train_loss=5525.1451, val_loss=18736.2000, train_acc=0.6360, val_acc=0.6445\n",
      "Epoch 53: train_loss=5213.1949, val_loss=18658.9587, train_acc=0.6465, val_acc=0.6671\n",
      "Epoch 54: train_loss=5430.3969, val_loss=15800.0233, train_acc=0.6467, val_acc=0.6497\n",
      "Epoch 55: train_loss=7365.9945, val_loss=16810.7840, train_acc=0.6360, val_acc=0.6492\n",
      "Epoch 56: train_loss=7113.5677, val_loss=15994.8156, train_acc=0.6265, val_acc=0.6653\n",
      "Epoch 57: train_loss=6412.8706, val_loss=17106.4826, train_acc=0.6368, val_acc=0.6613\n",
      "Epoch 58: train_loss=6261.5883, val_loss=16327.3108, train_acc=0.6301, val_acc=0.6590\n",
      "Epoch 59: train_loss=5926.7175, val_loss=15396.8821, train_acc=0.6351, val_acc=0.6492\n",
      "Epoch 60: train_loss=5160.8218, val_loss=16650.4863, train_acc=0.6376, val_acc=0.6515\n",
      "Epoch 61: train_loss=5708.8352, val_loss=16026.6792, train_acc=0.6400, val_acc=0.6544\n",
      "Epoch 62: train_loss=5299.3501, val_loss=17943.7896, train_acc=0.6537, val_acc=0.6653\n",
      "Epoch 63: train_loss=5055.6599, val_loss=15290.1427, train_acc=0.6423, val_acc=0.6572\n",
      "Epoch 64: train_loss=5144.4567, val_loss=16733.4595, train_acc=0.6456, val_acc=0.6705\n",
      "Epoch 65: train_loss=5019.5107, val_loss=18051.5576, train_acc=0.6545, val_acc=0.6636\n",
      "Epoch 66: train_loss=4524.1577, val_loss=18272.7994, train_acc=0.6477, val_acc=0.6619\n",
      "Epoch 67: train_loss=5253.2878, val_loss=16973.4983, train_acc=0.6576, val_acc=0.6584\n",
      "Epoch 68: train_loss=4870.2094, val_loss=17313.2076, train_acc=0.6641, val_acc=0.6855\n",
      "Epoch 69: train_loss=4527.1435, val_loss=18720.6859, train_acc=0.6664, val_acc=0.6440\n",
      "Epoch 70: train_loss=4993.6030, val_loss=19092.9826, train_acc=0.6640, val_acc=0.6872\n",
      "Epoch 71: train_loss=4816.6511, val_loss=17999.8148, train_acc=0.6663, val_acc=0.6601\n",
      "Epoch 72: train_loss=4483.8242, val_loss=19498.7320, train_acc=0.6582, val_acc=0.6780\n",
      "Epoch 73: train_loss=4153.8847, val_loss=19485.1789, train_acc=0.6588, val_acc=0.6647\n",
      "Epoch 74: train_loss=4427.6616, val_loss=19509.9508, train_acc=0.6585, val_acc=0.6682\n",
      "Epoch 75: train_loss=4312.9684, val_loss=17863.5859, train_acc=0.6552, val_acc=0.6624\n",
      "Epoch 76: train_loss=4013.5465, val_loss=18696.8947, train_acc=0.6581, val_acc=0.6515\n",
      "Epoch 77: train_loss=4455.8151, val_loss=18993.1152, train_acc=0.6617, val_acc=0.6728\n",
      "Epoch 78: train_loss=4485.1256, val_loss=20083.8856, train_acc=0.6631, val_acc=0.6711\n",
      "Epoch 79: train_loss=5005.8187, val_loss=18888.4953, train_acc=0.6611, val_acc=0.6769\n",
      "Epoch 80: train_loss=4773.0966, val_loss=17486.7298, train_acc=0.6585, val_acc=0.6676\n",
      "Epoch 81: train_loss=4080.2661, val_loss=18887.1990, train_acc=0.6609, val_acc=0.6751\n",
      "Epoch 82: train_loss=4101.5847, val_loss=21049.0182, train_acc=0.6657, val_acc=0.6913\n",
      "Epoch 83: train_loss=4431.8484, val_loss=21731.3084, train_acc=0.6591, val_acc=0.6665\n",
      "Epoch 84: train_loss=4164.7912, val_loss=26680.6023, train_acc=0.6630, val_acc=0.6890\n",
      "Epoch 85: train_loss=4286.4858, val_loss=22268.6674, train_acc=0.6627, val_acc=0.6728\n",
      "Epoch 86: train_loss=4231.7382, val_loss=20841.3583, train_acc=0.6654, val_acc=0.6665\n",
      "Epoch 87: train_loss=3990.6258, val_loss=16748.7993, train_acc=0.6657, val_acc=0.6838\n",
      "Epoch 88: train_loss=4097.4463, val_loss=20971.2444, train_acc=0.6660, val_acc=0.6457\n",
      "Epoch 89: train_loss=3933.5383, val_loss=17959.3681, train_acc=0.6667, val_acc=0.6792\n",
      "Epoch 90: train_loss=4059.6071, val_loss=18675.2949, train_acc=0.6625, val_acc=0.6665\n",
      "Epoch 91: train_loss=4659.3679, val_loss=19935.8840, train_acc=0.6646, val_acc=0.6751\n",
      "Epoch 92: train_loss=3976.1340, val_loss=22284.2305, train_acc=0.6666, val_acc=0.6792\n",
      "Epoch 93: train_loss=4366.9490, val_loss=21066.1436, train_acc=0.6686, val_acc=0.6769\n",
      "Epoch 94: train_loss=4007.8895, val_loss=19630.3298, train_acc=0.6682, val_acc=0.6682\n",
      "Epoch 95: train_loss=4319.6081, val_loss=22842.2458, train_acc=0.6654, val_acc=0.6809\n",
      "Epoch 96: train_loss=4279.6256, val_loss=19389.7416, train_acc=0.6634, val_acc=0.6671\n",
      "Epoch 97: train_loss=4082.0901, val_loss=20633.9310, train_acc=0.6669, val_acc=0.6890\n",
      "Epoch 98: train_loss=4506.6749, val_loss=18423.1684, train_acc=0.6683, val_acc=0.6751\n",
      "Epoch 99: train_loss=4123.4329, val_loss=19964.6596, train_acc=0.6653, val_acc=0.6567\n",
      "Epoch 100: train_loss=4524.5142, val_loss=19899.6091, train_acc=0.6663, val_acc=0.6780\n",
      "Epoch 101: train_loss=4107.1753, val_loss=21031.9195, train_acc=0.6644, val_acc=0.6953\n",
      "Epoch 102: train_loss=4347.8438, val_loss=23575.8397, train_acc=0.6638, val_acc=0.6890\n",
      "Epoch 103: train_loss=4264.6561, val_loss=23431.7536, train_acc=0.6650, val_acc=0.6803\n",
      "Epoch 104: train_loss=3832.1481, val_loss=22039.8709, train_acc=0.6679, val_acc=0.6746\n",
      "Epoch 105: train_loss=4098.1903, val_loss=18799.2204, train_acc=0.6644, val_acc=0.6607\n",
      "Epoch 106: train_loss=4257.8847, val_loss=17977.3399, train_acc=0.6634, val_acc=0.6590\n",
      "Epoch 107: train_loss=4544.1464, val_loss=17150.6387, train_acc=0.6663, val_acc=0.6763\n",
      "Epoch 108: train_loss=4083.6615, val_loss=21107.2799, train_acc=0.6667, val_acc=0.6774\n",
      "Epoch 109: train_loss=3911.9426, val_loss=19487.9858, train_acc=0.6648, val_acc=0.6544\n",
      "Epoch 110: train_loss=3999.1440, val_loss=22631.1473, train_acc=0.6673, val_acc=0.6630\n",
      "Epoch 111: train_loss=3847.4778, val_loss=18273.9974, train_acc=0.6641, val_acc=0.6942\n",
      "Epoch 112: train_loss=4191.2270, val_loss=21880.1021, train_acc=0.6686, val_acc=0.6901\n",
      "Epoch 113: train_loss=4221.1121, val_loss=18632.5927, train_acc=0.6687, val_acc=0.6757\n",
      "Epoch 114: train_loss=3613.1296, val_loss=19244.2170, train_acc=0.6686, val_acc=0.6601\n",
      "Epoch 115: train_loss=4412.6530, val_loss=22303.6836, train_acc=0.6686, val_acc=0.6676\n",
      "Epoch 116: train_loss=3871.8158, val_loss=20209.2271, train_acc=0.6670, val_acc=0.6711\n",
      "Epoch 117: train_loss=3915.4789, val_loss=26263.0287, train_acc=0.6682, val_acc=0.6769\n",
      "Epoch 118: train_loss=4015.4852, val_loss=22697.4834, train_acc=0.6670, val_acc=0.6884\n",
      "Epoch 119: train_loss=4718.2452, val_loss=24877.8414, train_acc=0.6654, val_acc=0.6832\n",
      "Epoch 120: train_loss=3733.1715, val_loss=20648.9610, train_acc=0.6677, val_acc=0.6717\n",
      "Epoch 121: train_loss=4802.6213, val_loss=18170.5418, train_acc=0.6658, val_acc=0.6838\n",
      "Epoch 122: train_loss=4181.4695, val_loss=18704.2217, train_acc=0.6657, val_acc=0.6930\n",
      "Epoch 123: train_loss=4198.6719, val_loss=21463.5866, train_acc=0.6676, val_acc=0.6792\n",
      "Epoch 124: train_loss=3907.0796, val_loss=20589.5401, train_acc=0.6667, val_acc=0.6821\n",
      "Epoch 125: train_loss=4072.2664, val_loss=20330.2946, train_acc=0.6682, val_acc=0.6797\n",
      "Epoch 126: train_loss=4173.4697, val_loss=20717.5574, train_acc=0.6700, val_acc=0.6878\n",
      "Epoch 127: train_loss=3957.7893, val_loss=20428.5749, train_acc=0.6670, val_acc=0.6740\n",
      "Epoch 128: train_loss=3849.9494, val_loss=20034.3401, train_acc=0.6680, val_acc=0.6826\n",
      "Epoch 129: train_loss=3604.3555, val_loss=19832.1748, train_acc=0.6692, val_acc=0.6878\n",
      "Epoch 130: train_loss=4287.0897, val_loss=17634.8204, train_acc=0.6654, val_acc=0.6797\n",
      "Epoch 131: train_loss=3928.5741, val_loss=23828.9296, train_acc=0.6648, val_acc=0.6884\n",
      "Epoch 132: train_loss=3845.2735, val_loss=22151.5548, train_acc=0.6673, val_acc=0.6751\n",
      "Epoch 133: train_loss=4050.8526, val_loss=26018.8127, train_acc=0.6687, val_acc=0.6786\n",
      "Epoch 134: train_loss=3992.9046, val_loss=20360.5933, train_acc=0.6683, val_acc=0.6497\n",
      "Epoch 135: train_loss=3926.2541, val_loss=19896.0411, train_acc=0.6684, val_acc=0.6890\n",
      "Epoch 136: train_loss=4009.9672, val_loss=18996.3889, train_acc=0.6646, val_acc=0.6815\n",
      "Epoch 137: train_loss=4022.7606, val_loss=21028.4553, train_acc=0.6677, val_acc=0.6624\n",
      "Epoch 138: train_loss=4363.4802, val_loss=18985.6522, train_acc=0.6680, val_acc=0.6872\n",
      "Epoch 139: train_loss=4164.5394, val_loss=17481.0503, train_acc=0.6706, val_acc=0.6676\n",
      "Epoch 140: train_loss=4067.2379, val_loss=18810.5338, train_acc=0.6677, val_acc=0.6665\n",
      "Epoch 141: train_loss=4380.9055, val_loss=20994.2909, train_acc=0.6670, val_acc=0.6728\n",
      "Epoch 142: train_loss=4391.3831, val_loss=22049.5125, train_acc=0.6669, val_acc=0.6711\n",
      "Epoch 143: train_loss=3966.7322, val_loss=19799.5280, train_acc=0.6644, val_acc=0.6896\n",
      "Epoch 144: train_loss=4290.6681, val_loss=17494.4581, train_acc=0.6646, val_acc=0.6549\n",
      "Epoch 145: train_loss=4353.3844, val_loss=22125.7873, train_acc=0.6656, val_acc=0.6855\n",
      "Epoch 146: train_loss=4056.9635, val_loss=20296.0745, train_acc=0.6648, val_acc=0.6867\n",
      "Epoch 147: train_loss=4265.2068, val_loss=21623.0234, train_acc=0.6620, val_acc=0.6832\n",
      "Epoch 148: train_loss=4144.1835, val_loss=21024.3339, train_acc=0.6656, val_acc=0.6699\n",
      "Epoch 149: train_loss=4489.1548, val_loss=20232.9450, train_acc=0.6646, val_acc=0.6878\n",
      "Epoch 150: train_loss=3830.0642, val_loss=18963.5381, train_acc=0.6640, val_acc=0.6642\n",
      "Validation Accuracy: 0.6642\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Non-Repeater       0.20      0.97      0.33       150\n",
      "    Repeater       1.00      0.63      0.78      1583\n",
      "\n",
      "    accuracy                           0.66      1733\n",
      "   macro avg       0.60      0.80      0.55      1733\n",
      "weighted avg       0.93      0.66      0.74      1733\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 146    4]\n",
      " [ 578 1005]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, recall_score\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# device & matplotlib config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "matplotlib.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "})\n",
    "\n",
    "# helper to fill repeater based on source\n",
    "def fill_repeater_from_source(row, data):\n",
    "    if row['Source'] == 'FRB20220912A':\n",
    "        return 1\n",
    "    else:\n",
    "        return row['Repeater']\n",
    "\n",
    "# clean numeric strings → float\n",
    "def clean_numeric_value(value):\n",
    "    if isinstance(value, str):\n",
    "        v = value.strip()\n",
    "        if not v:\n",
    "            return np.nan\n",
    "        for ch in ['/', '+', '<', '>', '~']:\n",
    "            v = v.replace(ch, '')\n",
    "        if '-' in v:\n",
    "            v = v.split('-')[0]\n",
    "        try:\n",
    "            return float(v)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan\n",
    "\n",
    "# 1) Load & initial repeater processing\n",
    "frb_data = pd.read_csv('frb-data.csv')\n",
    "frb_data['Repeater'] = frb_data['Repeater'].map({'Yes':1,'No':0})\n",
    "frb_data['Repeater'] = frb_data['Repeater'].fillna(0).astype(int)\n",
    "frb_data['Repeater'] = frb_data.apply(fill_repeater_from_source, axis=1, data=frb_data)\n",
    "labels = frb_data['Repeater']\n",
    "\n",
    "# 2) Extract raw per-sample SNR for weighting\n",
    "frb_data['SNR_raw'] = frb_data['SNR'].apply(clean_numeric_value).fillna(0)\n",
    "snr_array = frb_data['SNR_raw'].values\n",
    "\n",
    "# 3) Define features (exclude 'SNR')\n",
    "base_features = [\n",
    "    'Observing_band',\n",
    "    'Freq_high', 'Freq_low', 'Freq_peak',\n",
    "    'Width'\n",
    "]\n",
    "error_features = [\n",
    "    'DM_SNR','DM_alig','Flux_density','Fluence','Energy',\n",
    "    'Polar_l','Polar_c','RM_syn','RM_QUfit','Scatt_t'\n",
    "]\n",
    "\n",
    "# clean numeric for all features\n",
    "for feat in base_features + error_features:\n",
    "    frb_data[feat] = frb_data[feat].apply(clean_numeric_value)\n",
    "for feat in error_features:\n",
    "    frb_data[f'{feat}_err'] = frb_data[f'{feat}_err'].apply(clean_numeric_value)\n",
    "    frb_data[f'{feat}_upper'] = frb_data[feat] + frb_data[f'{feat}_err']\n",
    "    frb_data[f'{feat}_lower'] = (frb_data[feat] - frb_data[f'{feat}_err']).clip(lower=0)\n",
    "\n",
    "# build cleaned feature matrix\n",
    "features = (\n",
    "    base_features +\n",
    "    error_features +\n",
    "    [f'{f}_upper' for f in error_features] +\n",
    "    [f'{f}_lower' for f in error_features]\n",
    ")\n",
    "frb_data_clean = frb_data[features].fillna(0)\n",
    "\n",
    "# 4) Scale features\n",
    "scaler = StandardScaler()\n",
    "frb_data_scaled = scaler.fit_transform(frb_data_clean)\n",
    "\n",
    "# keep original indices\n",
    "indices = frb_data_clean.index\n",
    "\n",
    "# 5) Split into train/val, including per-sample SNR\n",
    "train_data, val_data, train_labels, val_labels, train_snr, val_snr, train_indices, val_indices = train_test_split(\n",
    "    frb_data_scaled, labels.values, snr_array, indices,\n",
    "    test_size=0.2, random_state=42, stratify=labels.values\n",
    ")\n",
    "\n",
    "# 6) To tensors & dataloaders (now yielding snr too)\n",
    "batch_size = 64\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "val_tensor   = torch.tensor(val_data,   dtype=torch.float32)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "val_labels_tensor   = torch.tensor(val_labels,   dtype=torch.long)\n",
    "train_snr_tensor = torch.tensor(train_snr, dtype=torch.float32)\n",
    "val_snr_tensor   = torch.tensor(val_snr,   dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_tensor, train_labels_tensor, train_snr_tensor)\n",
    "val_dataset   = TensorDataset(val_tensor,   val_labels_tensor,   val_snr_tensor)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader    = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 7) Model hyperparams\n",
    "input_dim   = val_tensor.shape[1]\n",
    "hidden_dim  = 256\n",
    "latent_dim  = 10\n",
    "stop_patience = 8\n",
    "num_epochs    = 150\n",
    "\n",
    "# define your VAE + classifier\n",
    "class SupervisedVAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, dropout_rate=0.3, activation=nn.LeakyReLU(0.1)):\n",
    "        super(SupervisedVAE, self).__init__()\n",
    "        self.activation = activation\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), self.activation, nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), self.activation, nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), self.activation, nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.fc_mu     = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), self.activation, nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), self.activation, nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim), self.activation, nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim//2),\n",
    "            self.activation, nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim//2, hidden_dim//4),\n",
    "            self.activation, nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim//4, 1),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        class_logit = self.classifier(mu)\n",
    "        return recon_x, mu, logvar, class_logit\n",
    "\n",
    "# 8) Per-sample SNR-weighted loss\n",
    "def loss_function(recon_x, x, mu, logvar, class_logit,\n",
    "                  labels, beta, gamma, class_weight, classification_multiplier,\n",
    "                  snr_batch):\n",
    "    # recon per-element → [B, D]\n",
    "    recon_elem = F.mse_loss(recon_x, x, reduction='none')\n",
    "    # sum over features → [B]\n",
    "    recon_per = recon_elem.sum(dim=1)\n",
    "    # KL per-sample → [B]\n",
    "    kl_per = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "    # classification per-sample → [B]\n",
    "    pos_w = torch.tensor([class_weight], device=device)\n",
    "    class_elem = F.binary_cross_entropy_with_logits(\n",
    "        class_logit, labels.unsqueeze(1).float(),\n",
    "        reduction='none', pos_weight=pos_w\n",
    "    ).squeeze(1) * classification_multiplier\n",
    "    # total per-sample\n",
    "    total_per = recon_per + beta * kl_per + gamma * class_elem\n",
    "    # weight by SNR and sum → scalar\n",
    "    weighted_loss = (snr_batch * total_per).sum()\n",
    "    # also sum raw terms for logging\n",
    "    recon_loss = recon_per.sum()\n",
    "    kl_loss    = kl_per.sum()\n",
    "    class_loss = class_elem.sum()\n",
    "    return weighted_loss, recon_loss, kl_loss, class_loss\n",
    "\n",
    "# 9) Training & validation loops\n",
    "def train_supervised(model, optimizer, scheduler, epoch, beta, gamma, class_weight, classification_multiplier):\n",
    "    model.train()\n",
    "    total_loss = total_recon = total_kl = total_class = 0\n",
    "    correct = total = 0\n",
    "    for batch_idx, (data, labels, snr) in enumerate(train_loader):\n",
    "        data, labels, snr = data.to(device), labels.to(device), snr.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar, class_logit = model(data)\n",
    "        loss, recon_l, kl_l, class_l = loss_function(\n",
    "            recon_batch, data, mu, logvar, class_logit,\n",
    "            labels, beta, gamma, class_weight, classification_multiplier,\n",
    "            snr\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss      += loss.item()\n",
    "        total_recon     += recon_l.item()\n",
    "        total_kl        += kl_l.item()\n",
    "        total_class     += class_l.item()\n",
    "        preds = (torch.sigmoid(class_logit) > 0.5).float().squeeze(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total   += labels.size(0)\n",
    "    return (total_loss/len(train_loader.dataset),\n",
    "            total_recon/len(train_loader.dataset),\n",
    "            total_kl/len(train_loader.dataset),\n",
    "            total_class/len(train_loader.dataset),\n",
    "            correct/total)\n",
    "\n",
    "def validate_supervised(model, scheduler, optimizer, epoch, beta, gamma, class_weight, classification_multiplier):\n",
    "    model.eval()\n",
    "    val_loss = val_recon = val_kl = val_class = 0\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels, snr in val_loader:\n",
    "            data, labels, snr = data.to(device), labels.to(device), snr.to(device)\n",
    "            recon_batch, mu, logvar, class_logit = model(data)\n",
    "            loss, recon_l, kl_l, class_l = loss_function(\n",
    "                recon_batch, data, mu, logvar, class_logit,\n",
    "                labels, beta, gamma, class_weight, classification_multiplier,\n",
    "                snr\n",
    "            )\n",
    "            val_loss   += loss.item()\n",
    "            val_recon  += recon_l.item()\n",
    "            val_kl     += kl_l.item()\n",
    "            val_class  += class_l.item()\n",
    "            preds = (torch.sigmoid(class_logit) > 0.5).float().squeeze(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total   += labels.size(0)\n",
    "    return (val_loss/len(val_loader.dataset),\n",
    "            val_recon/len(val_loader.dataset),\n",
    "            val_kl/len(val_loader.dataset),\n",
    "            val_class/len(val_loader.dataset),\n",
    "            correct/total)\n",
    "\n",
    "# 10) Early stopping helper\n",
    "def early_stopping(val_losses, patience):\n",
    "    if len(val_losses) > patience:\n",
    "        if all(val_losses[-i-1] <= val_losses[-i] for i in range(1, patience+1)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 11) Hyperparams and training\n",
    "best_params = {\n",
    "    'hidden_dim': 1082,\n",
    "    'latent_dim': 18,\n",
    "    'beta': 1.149574612306723,\n",
    "    'gamma': 1.9210647260496314,\n",
    "    'dropout_rate': 0.13093239424733344,\n",
    "    'lr': 0.0011823749066137313,\n",
    "    'scheduler_patience': 7,\n",
    "    'class_weight': 0.35488674730648145,\n",
    "    'classification_multiplier': 7817.124805902009,\n",
    "    'activation': nn.ReLU()\n",
    "}\n",
    "\n",
    "model = SupervisedVAE(\n",
    "    input_dim, best_params['hidden_dim'], best_params['latent_dim'],\n",
    "    best_params['dropout_rate'], best_params['activation']\n",
    ").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min',\n",
    "                              factor=0.5,\n",
    "                              patience=best_params['scheduler_patience'])\n",
    "\n",
    "val_losses = []\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss, _, _, _, train_acc = train_supervised(\n",
    "        model, optimizer, scheduler, epoch,\n",
    "        best_params['beta'], best_params['gamma'],\n",
    "        best_params['class_weight'], best_params['classification_multiplier']\n",
    "    )\n",
    "    val_loss, _, _, _, val_acc = validate_supervised(\n",
    "        model, optimizer, scheduler, epoch,\n",
    "        best_params['beta'], best_params['gamma'],\n",
    "        best_params['class_weight'], best_params['classification_multiplier']\n",
    "    )\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n",
    "          f\"train_acc={train_acc:.4f}, val_acc={val_acc:.4f}\")\n",
    "    val_losses.append(val_loss)\n",
    "    if early_stopping(val_losses, stop_patience):\n",
    "        print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "# 12) Final evaluation\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for data, labels, _ in val_loader:\n",
    "        data = data.to(device)\n",
    "        logits = model(data)[-1]\n",
    "        preds = (torch.sigmoid(logits) > 0.5).cpu().numpy().squeeze()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "print(\"Classification Report:\\n\", classification_report(all_labels, all_preds,\n",
    "                                                      target_names=[\"Non-Repeater\",\"Repeater\"]))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".frb-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
